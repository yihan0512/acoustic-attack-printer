{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d154c5-1466-45f1-8991-de30fb946371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import *\n",
    "from model import BiLSTM\n",
    "from CheckpointWriter.CheckpointWriter import CheckpointWriter\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0eedf2-fd6d-4bac-8732-cdf811ffe1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def load_audio(path):\n",
    "    return torchaudio.load(path)[0]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    transposed = list(zip(*batch))\n",
    "    samples = transposed[0]\n",
    "    targets = torch.LongTensor(transposed[1])\n",
    "    return [samples, targets]\n",
    "\n",
    "def sort_batch(batch):\n",
    "    batch_length = list(map(len, batch))\n",
    "    perm_idx = sorted(list(range(len(batch_length))), key=lambda k: batch_length[k], reverse=True)\n",
    "    batch_sorted = [batch[perm_idx[i]].squeeze(0).T for i in range(len(batch_length))]\n",
    "    return batch_sorted, perm_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3967b71-7b6a-43d7-ab97-0b70e7f679c5",
   "metadata": {},
   "source": [
    "## prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c71e0bb-f456-41b8-aba6-bcce445384e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device_id = '2'\n",
    "device = torch.device('cuda:'+device_id)\n",
    "\n",
    "# parameters\n",
    "n_fft = 512\n",
    "input_size = n_fft // 2 + 1\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.03\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "expr_log = 'fft_128_2'\n",
    "data_dir = os.path.join('data', 'F2000_split')\n",
    "\n",
    "# data loader\n",
    "spectrogram = torchaudio.transforms.Spectrogram(n_fft=n_fft)\n",
    "transform = transforms.Compose([\n",
    "                        spectrogram,\n",
    "                        ])\n",
    "train_set = DatasetFolder(root=os.path.join(data_dir, 'train'), \n",
    "                        loader=load_audio, \n",
    "                        extensions=['wav'],\n",
    "                        transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "test_set = DatasetFolder(root=os.path.join(data_dir, 'test'), \n",
    "                        loader=load_audio, \n",
    "                        extensions=['wav'],\n",
    "                        transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "# model\n",
    "num_classes = len(train_set.classes)\n",
    "model = BiLSTM(input_size, hidden_size, num_layers, num_classes, device_id=device_id).to(device)\n",
    "\n",
    "# checkpoint\n",
    "model_specs = {}\n",
    "model_specs['base'] = {}\n",
    "model_specs['base']['input_size'] = input_size\n",
    "# model_specs['base']['max_len'] = max_len \n",
    "model_specs['base']['hidden_size'] = hidden_size\n",
    "model_specs['base']['num_layers'] = num_layers\n",
    "model_specs['base']['num_classes'] = num_classes\n",
    "# model_specs['base']['feature_choice'] = feature_choice\n",
    "checkpoint = CheckpointWriter(os.path.join('runs', expr_log), model_specs)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', verbose=True, patience=100)\n",
    "\n",
    "# Tensorboard\n",
    "writer = SummaryWriter(os.path.join('runs', expr_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8149b-563b-49f5-8093-f82b07e1e075",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82ac2cd-ba83-48dd-8f5e-de6541a466d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7a165646014cceaed8763e2fe8934a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100, 0%:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f1707367c8c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimages_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         images_packed = nn.utils.rnn.pack_padded_sequence(nn.utils.rnn.pad_sequence(images_sorted, batch_first=True), \n\u001b[0m\u001b[1;32m     13\u001b[0m                                                           \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages_sorted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                           \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/acoustic/lib/python3.7/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;31m# assuming trailing dimensions and type of all the Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;31m# in sequences are same and fetching those from sequences[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "best_val_accu = -1\n",
    "best_loss_val = 100\n",
    "loss_val = 100\n",
    "accuracy_val = 0\n",
    "t = trange(num_epochs, desc='{}, {}%'.format(loss_val, accuracy_val))\n",
    "for epoch in t:\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images_sorted, perm_idx = sort_batch(images)\n",
    "        images_packed = nn.utils.rnn.pack_padded_sequence(nn.utils.rnn.pad_sequence(images_sorted, batch_first=True), \n",
    "                                                          [v.size(0) for v in images_sorted], \n",
    "                                                          batch_first=True,\n",
    "                                                          enforce_sorted=False)\n",
    "        images_packed = images_packed.to(device)\n",
    "        labels_sorted = labels[perm_idx]\n",
    "        labels_sorted = labels_sorted.to(device)\n",
    "\n",
    "\n",
    "        # images = images.to(device)\n",
    "        # labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images_packed, len(images))\n",
    "        loss = criterion(outputs, labels_sorted)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.softmax(dim=1), dim=1)\n",
    "        correct += (predicted == labels_sorted).sum()\n",
    "    accuracy = 100 * torch.true_divide(correct, train_set.__len__()) \n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_val = 0\n",
    "        for images, labels in test_loader:\n",
    "            images_sorted, perm_idx = sort_batch(images)\n",
    "            images_packed = nn.utils.rnn.pack_padded_sequence(nn.utils.rnn.pad_sequence(images_sorted, batch_first=True), \n",
    "                                                              [v.size(0) for v in images_sorted], \n",
    "                                                              batch_first=True,\n",
    "                                                              enforce_sorted=False)\n",
    "            images_packed = images_packed.to(device)\n",
    "            labels_sorted = labels[perm_idx]\n",
    "            labels_sorted = labels_sorted.to(device)\n",
    "            # images = images.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            outputs= model(images_packed, len(images))\n",
    "            loss_val = criterion(outputs, labels_sorted)\n",
    "            _, predicted = torch.max(outputs.softmax(dim=1), 1)\n",
    "            correct_val += (predicted == labels_sorted).sum()\n",
    "        accuracy_val = 100 * torch.true_divide(correct_val, test_set.__len__())\n",
    "\n",
    "    # Learning rate adjustment\n",
    "    scheduler.step(loss_val)\n",
    "    \n",
    "    # Tensorboard update\n",
    "    writer.add_scalar('loss/train', loss.item(), epoch+1)\n",
    "    writer.add_scalar('accuracy/train', accuracy, epoch+1)\n",
    "    writer.add_scalar('loss/validation', loss_val.item(), epoch+1)\n",
    "    writer.add_scalar('accuracy/validation', accuracy_val, epoch+1)\n",
    "    writer.add_scalar('Learning rate', optimizer.param_groups[0]['lr'], epoch+1)\n",
    "    \n",
    "    # tqdm update\n",
    "    t.set_description('{:.4f}, {:.2f}%'.format(loss_val.item(), accuracy_val))\n",
    "    \n",
    "    # Update model with best validation accuracy\n",
    "    if loss_val < best_loss_val:\n",
    "        best_loss_val = loss_val\n",
    "        checkpoint.save(model, optimizer, accuracy_val, loss_val, epoch, loss)\n",
    "\n",
    "# close tensorboard\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
